{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentación 3D del bazo con MONAI\n",
    "\n",
    "Este tutorial muestra cómo integrar MONAI en un programa existente de deep learning médico con PyTorch.\n",
    "\n",
    "Y usar fácilmente las siguientes características:\n",
    "1. Transforms para datos en formato diccionario.\n",
    "1. Cargar imágenes Nifti con metadatos.\n",
    "1. Añadir dimensión de canal a los datos si no la tienen.\n",
    "1. Escalar la intensidad de la imagen médica al rango esperado.\n",
    "1. Recortar un lote de imágenes balanceadas según la proporción de etiquetas positivas/negativas.\n",
    "1. Cachear IO y transforms para acelerar el entrenamiento y la validación.\n",
    "1. Modelo UNet 3D, función de pérdida Dice, métrica Dice media para tareas de segmentación 3D.\n",
    "1. Método de inferencia por ventana deslizante.\n",
    "1. Entrenamiento determinista para reproducibilidad.\n",
    "\n",
    "El dataset de bazo puede descargarse de http://medicaldecathlon.com/.\n",
    "\n",
    "Objetivo: Bazo  \n",
    "Modalidad: TC  \n",
    "Tamaño: 61 volúmenes 3D (41 entrenamiento + 20 prueba)  \n",
    "Fuente: Memorial Sloan Kettering Cancer Center  \n",
    "Desafío: Tamaño de foreground muy variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conocimiento necesario\n",
    "\n",
    "[Escala de Hounsfield](https://en.wikipedia.org/wiki/Hounsfield_scale)\n",
    "[Spacing](https://dicom.innolitics.com/ciods/rt-dose/image-plane/00280030)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar directorio de datos\n",
    "\n",
    "Puedes especificar un directorio con la variable de entorno `MONAI_DATA_DIRECTORY`.  \n",
    "Esto te permite guardar resultados y reutilizar descargas.  \n",
    "Si no se especifica, se usará un directorio temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargar dataset\n",
    "\n",
    "Descarga y extrae el dataset.  \n",
    "El dataset proviene de http://medicaldecathlon.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
    "data_dir = os.path.join(root_dir, \"Task09_Spleen\")\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir ruta del dataset MSD Spleen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "train_labels = sorted(glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar entrenamiento determinista para reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurar transforms para entrenamiento y validación\n",
    "\n",
    "Aquí usamos varios transforms para aumentar el dataset:\n",
    "1. `LoadImaged` carga las imágenes y etiquetas de TC del bazo desde archivos en formato NIfTI.\n",
    "1. `EnsureChannelFirstd` asegura que los datos originales tengan la forma \"canal primero\".\n",
    "1. `Orientationd` unifica la orientación de los datos según la matriz afín.\n",
    "1. `Spacingd` ajusta el espaciado con `pixdim=(1.5, 1.5, 2.)` según la matriz afín.\n",
    "1. `ScaleIntensityRanged` extrae el rango de intensidad [-57, 164] y lo escala a [0, 1].\n",
    "1. `CropForegroundd` elimina los bordes de ceros para enfocarse en el área válida del cuerpo en las imágenes y etiquetas.\n",
    "1. `RandCropByPosNegLabeld` recorta aleatoriamente parches de la imagen grande según la proporción pos/neg.  \n",
    "Los centros de imagen de las muestras negativas deben estar en el área válida del cuerpo.\n",
    "1. `RandAffined` realiza de manera eficiente `rotate`, `scale`, `shear`, `translate`, etc. juntos usando la transformación afín de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_smaller=True),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        # user can also add other random transforms\n",
    "        # RandAffined(\n",
    "        #     keys=['image', 'label'],\n",
    "        #     mode=('bilinear', 'nearest'),\n",
    "        #     prob=1.0, spatial_size=(96, 96, 96),\n",
    "        #     rotate_range=(0, 0, np.pi/15),\n",
    "        #     scale_range=(0.1, 0.1, 0.1)),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_smaller=True),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar transforms en DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
    "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "# plot the slice [:, :, 80]\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(image[:, :, 80], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, 80])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir CacheDataset y DataLoader para entrenamiento y validación\n",
    "\n",
    "Aquí usamos CacheDataset para acelerar el proceso de entrenamiento y validación, es 10 veces más rápido que el Dataset regular.  \n",
    "Para lograr el mejor rendimiento, establece `cache_rate=1.0` para cachear todos los datos; si no hay suficiente memoria, usa un valor menor.  \n",
    "Los usuarios también pueden establecer `cache_num` en vez de `cache_rate`, se usará el valor mínimo de ambos.  \n",
    "Y establece `num_workers` para habilitar multihilo durante el cacheo.  \n",
    "Si quieres probar el Dataset regular, solo cambia para usar el código comentado abajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "# train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld\n",
    "# to generate 2 x 4 images for network training\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear modelo, función de pérdida y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch program style: create UNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar un proceso típico de entrenamiento en PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                roi_size = (160, 160, 160)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                # compute metric for current iteration\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "\n",
    "            metric_values.append(metric)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                f\"at epoch: {best_metric_epoch}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficar la pérdida y la métrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificar la salida del mejor modelo con la imagen y etiqueta de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, val_data in enumerate(val_loader):\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, model)\n",
    "        # plot the slice [:, :, 80]\n",
    "        plt.figure(\"check\", (18, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f\"image {i}\")\n",
    "        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(f\"label {i}\")\n",
    "        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(f\"output {i}\")\n",
    "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
    "        plt.show()\n",
    "        if i == 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación en el espaciado original de la imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\", allow_smaller=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_org_ds = Dataset(data=val_files, transform=val_org_transforms)\n",
    "val_org_loader = DataLoader(val_org_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "post_transforms = Compose(\n",
    "    [\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=val_org_transforms,\n",
    "            orig_keys=\"image\",\n",
    "            meta_keys=\"pred_meta_dict\",\n",
    "            orig_meta_keys=\"image_meta_dict\",\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "            device=\"cpu\",\n",
    "        ),\n",
    "        AsDiscreted(keys=\"pred\", argmax=True, to_onehot=2),\n",
    "        AsDiscreted(keys=\"label\", to_onehot=2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_data in val_org_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        val_data[\"pred\"] = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "        val_data = [post_transforms(i) for i in decollate_batch(val_data)]\n",
    "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
    "        # compute metric for current iteration\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "    # aggregate the final mean dice result\n",
    "    metric_org = dice_metric.aggregate().item()\n",
    "    # reset the status for next validation round\n",
    "    dice_metric.reset()\n",
    "\n",
    "print(\"Metric on original image spacing: \", metric_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTs\", \"*.nii.gz\")))\n",
    "\n",
    "test_data = [{\"image\": image} for image in test_images]\n",
    "\n",
    "\n",
    "test_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=\"image\"),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\", allow_smaller=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_org_ds = Dataset(data=test_data, transform=test_org_transforms)\n",
    "\n",
    "test_org_loader = DataLoader(test_org_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "post_transforms = Compose(\n",
    "    [\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=test_org_transforms,\n",
    "            orig_keys=\"image\",\n",
    "            meta_keys=\"pred_meta_dict\",\n",
    "            orig_meta_keys=\"image_meta_dict\",\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "        ),\n",
    "        AsDiscreted(keys=\"pred\", argmax=True, to_onehot=2),\n",
    "        SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=\"./out\", output_postfix=\"seg\", resample=False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment the following lines to visualize the predicted results\n",
    "# from monai.transforms import LoadImage\n",
    "# loader = LoadImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_org_loader:\n",
    "        test_inputs = test_data[\"image\"].to(device)\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        test_data[\"pred\"] = sliding_window_inference(test_inputs, roi_size, sw_batch_size, model)\n",
    "\n",
    "        test_data = [post_transforms(i) for i in decollate_batch(test_data)]\n",
    "\n",
    "#         # uncomment the following lines to visualize the predicted results\n",
    "#         test_output = from_engine([\"pred\"])(test_data)\n",
    "\n",
    "#         original_image = loader(test_output[0].meta[\"filename_or_obj\"])\n",
    "\n",
    "#         plt.figure(\"check\", (18, 6))\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.imshow(original_image[:, :, 20], cmap=\"gray\")\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.imshow(test_output[0].detach().cpu()[1, :, :, 20])\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpiar directorio de datos\n",
    "\n",
    "Eliminar el directorio si se usó uno temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medicalimage-pgtd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
